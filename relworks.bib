@inproceedings{Veldema-LCPC-2007,
author = {Veldema, Ronald and Philippsen, Michael},
year = {2007},
month = {10},
pages = {217-231},
title = {Evaluation of RDMA opportunities in an object-oriented DSM},
isbn = {978-3-540-85260-5},
doi = {10.1007/978-3-540-85261-2_15}
}
@article{Cai-VLDB-2018,
author = {Cai, Qingchao and Guo, Wentian and Zhang, Hao and Agrawal, Divyakant and Chen, Gang and Ooi, Beng Chin and Tan, Kian-Lee and Teo, Yong Meng and Wang, Sheng},
title = {Efficient distributed memory management with RDMA and caching},
year = {2018},
issue_date = {July 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3236187.3236209},
doi = {10.14778/3236187.3236209},
abstract = {Recent advancements in high-performance networking interconnect significantly narrow the performance gap between intra-node and inter-node communications, and open up opportunities for distributed memory platforms to enforce cache coherency among distributed nodes. To this end, we propose GAM, an efficient distributed in-memory platform that provides a directory-based cache coherence protocol over remote direct memory access (RDMA). GAM manages the free memory distributed among multiple nodes to provide a unified memory model, and supports a set of user-friendly APIs for memory operations. To remove writes from critical execution paths, GAM allows a write to be reordered with the following reads and writes, and hence enforces partial store order (PSO) memory consistency. A light-weight logging scheme is designed to provide fault tolerance in GAM. We further build a transaction engine and a distributed hash table (DHT) atop GAM to show the ease-of-use and applicability of the provided APIs. Finally, we conduct an extensive micro benchmark to evaluate the read/write/lock performance of GAM under various workloads, and a macro benchmark against the transaction engine and DHT. The results show the superior performance of GAM over existing distributed memory platforms.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {1604–1617},
numpages = {14}
}
@inproceedings{Taranov-ICMD-2021,
author = {Taranov, Konstantin and Di Girolamo, Salvatore and Hoefler, Torsten},
title = {CoRM: Compactable Remote Memory over RDMA},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3452817},
doi = {10.1145/3448016.3452817},
abstract = {Distributed memory systems are becoming increasingly important since they provide a system-scale abstraction where physically separated memories can be addressed as a single logical one. This abstraction enables memory disaggregation, allowing systems as in-memory databases, caching services, and ephemeral storage to be naturally deployed at large scales. While this abstraction effectively increases the memory capacity of these systems, it faces additional overheads for remote memory accesses. To narrow the difference between local and remote accesses, low latency RDMA networks are a key element for efficient memory disaggregation. However, RDMA acceleration poses new obstacles to efficient memory management and particularly to memory compaction: network controllers and CPUs can concurrently access memory, potentially leading to inconsistencies if memory management operations are not synchronized. To ensure consistency, most distributed memory systems do not provide memory compaction and are exposed to memory fragmentation. We introduce CoRM, an RDMA-accelerated shared memory system that supports memory compaction and ensures strict consistency while providing one-sided RDMA accesses. We show that CoRM sustains high read throughput during normal operations, comparable to similar systems not providing memory compaction while experiencing minimal overheads during compaction. CoRM never disrupts RDMA connections and can reduce applications' active memory up to 6x by performing memory compaction.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {1811–1824},
numpages = {14},
keywords = {memory allocation, memory compaction, rdma},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@article{Wang-ACO-2024,
author = {Wang, Zhonghua and Guo, Yixing and Lu, Kai and Wan, Jiguang and Wang, Daohui and Yao, Ting and Wu, Huatao},
title = {Rcmp: Reconstructing RDMA-Based Memory Disaggregation via CXL},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3634916},
doi = {10.1145/3634916},
abstract = {Memory disaggregation is a promising architecture for modern datacenters that separates compute and memory resources into independent pools connected by ultra-fast networks, which can improve memory utilization, reduce cost, and enable elastic scaling of compute and memory resources. However, existing memory disaggregation solutions based on remote direct memory access (RDMA) suffer from high latency and additional overheads including page faults and code refactoring. Emerging cache-coherent interconnects such as CXL offer opportunities to reconstruct high-performance memory disaggregation. However, existing CXL-based approaches have physical distance limitation and cannot be deployed across racks.In this article, we propose Rcmp, a novel low-latency and highly scalable memory disaggregation system based on RDMA and CXL. The significant feature is that Rcmp improves the performance of RDMA-based systems via CXL, and leverages RDMA to overcome CXL’s distance limitation. To address the challenges of the mismatch between RDMA and CXL in terms of granularity, communication, and performance, Rcmp (1) provides a global page-based memory space management and enables fine-grained data access, (2) designs an efficient communication mechanism to avoid communication blocking issues, (3)&nbsp;proposes a hot-page identification and swapping strategy to reduce RDMA communications, and (4) designs an RDMA-optimized RPC framework to accelerate RDMA transfers. We implement a prototype of Rcmp and evaluate its performance by using micro-benchmarks and running a key-value store with YCSB benchmarks. The results show that Rcmp can achieve 5.2\texttimes{} lower latency and 3.8\texttimes{} higher throughput than RDMA-based systems. We also demonstrate that Rcmp can scale well with the increasing number of nodes without compromising performance.},
journal = {ACM Trans. Archit. Code Optim.},
month = jan,
articleno = {15},
numpages = {26},
keywords = {Memory disaggregation, RDMA, CXL}
}
@inproceedings{Dragojevic-NSDI-2014,
author = {Dragojevi\'{c}, Aleksandar and Narayanan, Dushyanth and Hodson, Orion and Castro, Miguel},
title = {FaRM: fast remote memory},
year = {2014},
isbn = {9781931971096},
publisher = {USENIX Association},
address = {USA},
abstract = {We describe the design and implementation of FaRM, a new main memory distributed computing platform that exploits RDMA to improve both latency and throughput by an order of magnitude relative to state of the art main memory systems that use TCP/IP. FaRM exposes the memory of machines in the cluster as a shared address space. Applications can use transactions to allocate, read, write, and free objects in the address space with location transparency. We expect this simple programming model to be sufficient for most application code. FaRM provides two mechanisms to improve performance where required: lock-free reads over RDMA, and support for collocating objects and function shipping to enable the use of efficient single machine transactions. FaRM uses RDMA both to directly access data in the shared address space and for fast messaging and is carefully tuned for the best RDMA performance. We used FaRM to build a key-value store and a graph store similar to Facebook's. They both perform well, for example, a 20-machine cluster can perform 167 million key-value lookups per second with a latency of 31µs.},
booktitle = {Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation},
pages = {401–414},
numpages = {14},
location = {Seattle, WA},
series = {NSDI'14}
}

@INPROCEEDINGS{Endo-IPDRM-2020,
  author={Endo, Wataru and Sato, Shigeyuki and Taura, Kenjiro},
  booktitle={2020 IEEE/ACM Fourth Annual Workshop on Emerging Parallel and Distributed Runtime Systems and Middleware (IPDRM)}, 
  title={MENPS: A Decentralized Distributed Shared Memory Exploiting RDMA}, 
  year={2020},
  volume={},
  number={},
  pages={9-16},
  keywords={Protocols;Merging;Runtime;Coherence;Synchronization;Program processors;Libraries;distributed shared memory;cache coherence protocol;home migration;timestamp based coherence;RDMA},
  doi={10.1109/IPDRM51949.2020.00006}}

@article{Hong-JCST-2019,
  title={Scaling out numa-aware applications with rdma-based distributed shared memory},
  author={Hong, Yang and Zheng, Yang and Yang, Fan and Zang, Bin-Yu and Guan, Hai-Bing and Chen, Hai-Bo},
  journal={Journal of Computer Science and Technology},
  volume={34},
  pages={94--112},
  year={2019},
  publisher={Springer}
}

@article{Jia-ACO-2022,
author = {Jia, Xingguo and Zhang, Jin and Yu, Boshi and Qian, Xingyue and Qi, Zhengwei and Guan, Haibing},
title = {GiantVM: A Novel Distributed Hypervisor for Resource Aggregation with DSM-aware Optimizations},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/3505251},
doi = {10.1145/3505251},
abstract = {We present GiantVM,1 an open-source distributed hypervisor that provides the many-to-one virtualization to aggregate resources from multiple physical machines. We propose techniques to enable distributed CPU and I/O virtualization and distributed shared memory (DSM) to achieve memory aggregation. GiantVM is implemented based on the state-of-the-art type-II hypervisor QEMU-KVM, and it can currently host conventional OSes such as Linux. (1) We identify the performance bottleneck of GiantVM to be DSM, through a top-down performance analysis. Although GiantVM offers great opportunities for CPU-intensive applications to enjoy the aggregated CPU resources, memory-intensive applications could suffer from cross-node page sharing, which requires frequent DSM involvement and leads to performance collapse. We design the guest-level thread scheduler, DaS (DSM-aware Scheduler), to overcome the bottleneck. When benchmarking with NAS Parallel Benchmarks, the DaS could achieve a performance boost of up to 3.5\texttimes{}, compared to the default Linux kernel scheduler. (2) While evaluating DaS, we observe the advantage of GiantVM as a resource reallocation facility. Thanks to the SSI abstraction of GiantVM, migration could be done by guest-level scheduling. DSM allows standby pages in the migration destination, which need not be transferred through the network. The saved network bandwidth is 68\% on average, compared to VM live migration. Resource reallocation with GiantVM increases the overall CPU utilization by 14.3\% in a co-location experiment.},
journal = {ACM Trans. Archit. Code Optim.},
month = mar,
articleno = {20},
numpages = {27},
keywords = {false sharing, distributed shared memory, single system image, virtualization, Resource aggregation}
}

@INPROCEEDINGS{Guedes-WWOSIII-1993,
  author={Guedes, P. and Castro, M.},
  booktitle={Proceedings of IEEE 4th Workshop on Workstation Operating Systems. WWOS-III}, 
  title={Distributed shared object memory}, 
  year={1993},
  volume={},
  number={},
  pages={142-149},
  keywords={Coherence;Programming profession;Yarn;Power system modeling;Prefetching;Hardware;High-speed networks;Workstations;Libraries;Drives},
  doi={10.1109/WWOS.1993.348158}}

@INPROCEEDINGS{Duan-ICDCS-2021,
  author={Duan, Zhuohui and Liu, Haikun and Lu, Haodi and Liao, Xiaofei and Jin, Hai and Zhang, Yu and He, Bingsheng},
  booktitle={2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)}, 
  title={Gengar: An RDMA-based Distributed Hybrid Memory Pool}, 
  year={2021},
  volume={},
  number={},
  pages={92-103},
  keywords={Data centers;Protocols;Costs;Nonvolatile memory;Conferences;Semantics;Random access memory;Distributed Shared Memory;Non-volatile Memory;RDMA;Persistent Memory},
  doi={10.1109/ICDCS51616.2021.00018}}

@inproceedings{Tsai-SOSP-2017,
author = {Tsai, Shin-Yeh and Zhang, Yiying},
title = {LITE Kernel RDMA Support for Datacenter Applications},
year = {2017},
isbn = {9781450350853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132747.3132762},
doi = {10.1145/3132747.3132762},
abstract = {Recently, there is an increasing interest in building data-center applications with RDMA because of its low-latency, high-throughput, and low-CPU-utilization benefits. However, RDMA is not readily suitable for datacenter applications. It lacks a flexible, high-level abstraction; its performance does not scale; and it does not provide resource sharing or flexible protection. Because of these issues, it is difficult to build RDMA-based applications and to exploit RDMA's performance benefits.To solve these issues, we built LITE, a Local Indirection TiEr for RDMA in the Linux kernel that virtualizes native RDMA into a flexible, high-level, easy-to-use abstraction and allows applications to safely share resources. Despite the widely-held belief that kernel bypassing is essential to RDMA's low-latency performance, we show that using a kernel-level indirection can achieve both flexibility and low-latency, scalable performance at the same time. To demonstrate the benefits of LITE, we developed several popular datacenter applications on LITE, including a graph engine, a MapReduce system, a Distributed Shared Memory system, and a distributed atomic logging system. These systems are easy to build and deliver good performance. For example, our implementation of PowerGraph uses only 20 lines of LITE code, while outperforming PowerGraph by 3.5x to 5.6x.},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {306–324},
numpages = {19},
keywords = {RDMA, indirection, low-latency network, network stack},
location = {Shanghai, China},
series = {SOSP '17}
}
@inproceedings{Shamis-SIGMOD-2019,
author = {Shamis, Alex and Renzelmann, Matthew and Novakovic, Stanko and Chatzopoulos, Georgios and Dragojevi\'{c}, Aleksandar and Narayanan, Dushyanth and Castro, Miguel},
title = {Fast General Distributed Transactions with Opacity},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3300069},
doi = {10.1145/3299869.3300069},
abstract = {Transactions can simplify distributed applications by hiding data distribution, concurrency, and failures from the application developer. Ideally the developer would see the abstraction of a single large machine that runs transactions sequentially and never fails. This requires the transactional subsystem to provide opacity (strict serializability for both committed and aborted transactions), as well as transparent fault tolerance with high availability. As even the best abstractions are unlikely to be used if they perform poorly, the system must also provide high performance. Existing distributed transactional designs either weaken this abstraction or are not designed for the best performance within a data center. This paper extends the design of FaRM --- which provides strict serializability only for committed transactions --- to provide opacity while maintaining FaRM's high throughput, low latency, and high availability within a modern data center. It uses timestamp ordering based on real time with clocks synchronized to within tens of microseconds across a cluster, and a failover protocol to ensure correctness across clock master failures. FaRM with opacity can commit 5.4 million neworder transactions per second when running the TPC-C transaction mix on 90 machines with 3-way replication.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {433–448},
numpages = {16},
keywords = {rdma, opacity, multi-version concurrency control, global time, distributed transactions, clock synchronization},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}
@INPROCEEDINGS{Farreras-IPDPS-2009,
author = {Farreras, Montse and Almasi, George and Cascaval, Calin and Cortes, Toni},
title = {Scalable RDMA performance in PGAS languages},
year = {2009},
volume = {},
number = {},
pages = {1-12},
keywords = {Electronics packaging;Large-scale systems;Scalability;Runtime;Parallel programming;Yarn;Delay;Computer architecture;Parallel machines;Productivity},
doi = {10.1109/IPDPS.2009.5161025}
}

@ARTICLE{Amza-Usenix-1994,
author={Amza, C. and Cox, A.L. and Dwarkadas, S. and Keleher, P. and Honghui Lu and Rajamony, R. and Weimin Yu and Zwaenepoel, W.},
journal={Computer}, 
title={TreadMarks: shared memory computing on networks of workstations}, 
year={1996},
volume={29},
number={2},
pages={18-28},
keywords={Computer networks;Workstations;Hardware;Programming profession;Message passing;Data structures;Partitioning algorithms;Programming environments;Delay;Protection},
doi={10.1109/2.485843}}

@inproceedings{Houshmand-PLDI-2022,
author = {Houshmand, Farzin and Saberlatibari, Javad and Lesani, Mohsen},
title = {Hamband: RDMA replicated data types},
year = {2022},
isbn = {9781450392655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3519939.3523426},
doi = {10.1145/3519939.3523426},
abstract = {Data centers are increasingly equipped with RDMAs. These network interfaces mark the advent of a new distributed system model where a node can directly access the remote memory of another. They have enabled microsecond-scale replicated services. The underlying replication protocols of these systems execute all operations under strong consistency. However, strong consistency can hinder response time and availability, and recent replication models have turned to a hybrid of strong and relaxed consistency. This paper presents RDMA well-coordinated replicated data types, the first hybrid replicated data types for the RDMA network model. It presents a novel operational semantics for these data types that considers three distinct categories of methods and captures their required coordination, and formally proves that they preserve convergence and integrity. It implements these semantics in a system called Hamband that leverages direct remote accesses to efficiently implement the required coordination protocols. The empirical evaluation shows that Hamband outperforms the throughput of existing message-based and strongly consistent implementations by more than 17x and 2.7x respectively.},
booktitle = {Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {348–363},
numpages = {16},
keywords = {WRDT, RDMA, Operational Semantics},
location = {San Diego, CA, USA},
series = {PLDI 2022}
}

@inproceedings{Katsarakis-ASPLOS-2020,
author = {Katsarakis, Antonios and Gavrielatos, Vasilis and Katebzadeh, M.R. Siavash and Joshi, Arpit and Dragojevic, Aleksandar and Grot, Boris and Nagarajan, Vijay},
title = {Hermes: A Fast, Fault-Tolerant and Linearizable Replication Protocol},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378496},
doi = {10.1145/3373376.3378496},
abstract = {Today's datacenter applications are underpinned by datastores that are responsible for providing availability, consistency, and performance. For high availability in the presence of failures, these datastores replicate data across several nodes. This is accomplished with the help of a reliable replication protocol that is responsible for maintaining the replicas strongly-consistent even when faults occur. Strong consistency is preferred to weaker consistency models that cannot guarantee an intuitive behavior for the clients. Furthermore, to accommodate high demand at real-time latencies, datastores must deliver high throughput and low latency.This work introduces Hermes, a broadcast-based reliable replication protocol for in-memory datastores that provides both high throughput and low latency by enabling local reads and fully-concurrent fast writes at all replicas. Hermes couples logical timestamps with cache-coherence-inspired invalidations to guarantee linearizability, avoid write serialization at a centralized ordering point, resolve write conflicts locally at each replica (hence ensuring that writes never abort) and provide fault-tolerance via replayable writes. Our implementation of Hermes over an RDMA-enabled reliable datastore with five replicas shows that Hermes consistently achieves higher throughput than state-of-the-art RDMA-based reliable protocols (ZAB and CRAQ) across all write ratios while also significantly reducing tail latency. At 5\% writes, the tail latency of Hermes is 3.6X lower than that of CRAQ and ZAB.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {201–217},
numpages = {17},
keywords = {availability, consistency, fault-tolerant, latency, linearizability, rdma, replication, throughput},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}

@inproceedings{Gavrielatos-EuroSys-2021,
author = {Gavrielatos, Vasilis and Katsarakis, Antonios and Nagarajan, Vijay},
title = {Odyssey: the impact of modern hardware on strongly-consistent replication protocols},
year = {2021},
isbn = {9781450383349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447786.3456240},
doi = {10.1145/3447786.3456240},
abstract = {Get/Put Key-Value Stores (KVSes) rely on replication protocols to enforce consistency and guarantee availability. Today's modern hardware, with manycore servers and RDMA-capable networks, challenges the conventional wisdom on protocol design. In this paper, we investigate the impact of modern hardware on the performance of strongly-consistent replication protocols.First, we create an informal taxonomy of replication protocols, based on which we carefully select 10 protocols for analysis. Secondly, we present Odyssey, a framework tailored towards protocol implementation for multi-threaded, RDMA-enabled, in-memory, replicated KVSes. We implement all 10 protocols over Odyssey, and perform the first apples-to-apples comparison of replication protocols over modern hardware.Our comparison characterizes the protocol design space, revealing the performance capabilities of different classes of protocols on modern hardware. Among other things, our results demonstrate that some of the protocols that were efficient in yesterday's hardware are not so today because they cannot take advantage of the abundant parallelism and fast networking present in modern hardware. Conversely, some protocols that were inefficient in yesterday's hardware are very attractive today. We distill our findings in a concise set of general guidelines and recommendations for protocol selection and design in the era of modern hardware.},
booktitle = {Proceedings of the Sixteenth European Conference on Computer Systems},
pages = {245–260},
numpages = {16},
keywords = {RDMA, availability, consistency, fault-tolerant, latency, linearizability, replication, throughput},
location = {Online Event, United Kingdom},
series = {EuroSys '21}
}

@inproceedings{Kaxiras-HPDC-2015,
author = {Kaxiras, Stefanos and Klaftenegger, David and Norgren, Magnus and Ros, Alberto and Sagonas, Konstantinos},
title = {Turning Centralized Coherence and Distributed Critical-Section Execution on their Head: A New Approach for Scalable Distributed Shared Memory},
year = {2015},
isbn = {9781450335508},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2749246.2749250},
doi = {10.1145/2749246.2749250},
abstract = {A coherent global address space in a distributed system enables shared memory programming in a much larger scale than a single multicore or a single SMP. Without dedicated hardware support at this scale, the solution is a software distributed shared memory (DSM) system. However, traditional approaches to coherence (centralized via "active" home-node directories) and critical-section execution (distributed across nodes and cores) are inherently unfit for such a scenario. Instead, it is crucial to make decisions locally and avoid the long latencies imposed by both network and software message handlers. Likewise, synchronization is fast if it rarely involves communication with distant nodes (or even other sockets). To minimize the amount of long-latency communication required in both coherence and critical section execution, we propose a DSM system with a novel coherence protocol, and a novel hierarchical queue delegation locking approach. More specifically, we propose an approach, suitable for Data-Race-Free programs, based on self-invalidation, self-downgrade, and passive data classification directories that require no message handlers, thereby incurring no extra latency. For fast synchronization we extend Queue Delegation Locking to execute critical sections in large batches on a single core before passing execution along to other cores, sockets, or nodes, in that hierarchical order. The result is a software DSM system called Argo which localizes as many decisions as possible and allows high parallel performance with little overhead on synchronization when compared to prior DSM implementations.},
booktitle = {Proceedings of the 24th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {3–14},
numpages = {12},
location = {Portland, Oregon, USA},
series = {HPDC '15}
}

@inproceedings{Gavrielatos-PPoPP-2020,
author = {Gavrielatos, Vasilis and Katsarakis, Antonios and Nagarajan, Vijay and Grot, Boris and Joshi, Arpit},
title = {Kite: efficient and available release consistency for the datacenter},
year = {2020},
isbn = {9781450368186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332466.3374516},
doi = {10.1145/3332466.3374516},
abstract = {Key-Value Stores (KVSs) came into prominence as highly-available, eventually consistent (EC), "NoSQL" Databases, but have quickly transformed into general-purpose, programmable storage systems. Thus, EC, while relevant, is no longer sufficient. Complying with the emerging requirements for stronger consistency, researchers have proposed KVSs with multiple consistency levels (MCL) that expose the consistency/performance trade-off to the programmer. We argue that this approach falls short in both programmability and performance. For instance, the MCL APIs proposed thus far, fail to capture the ordering relationship between strongly- and weakly-consistent accesses that naturally occur in programs.Taking inspiration from shared memory, we advocate Release Consistency (RC) for KVSs. We argue that RC's onesided barriers are ideal for capturing the ordering relationship between synchronization and non-synchronization accesses while enabling high-performance.We present Kite, the first highly-available, replicated KVS that offers a linearizable variant of RC for the asynchronous setting with individual process and network failures. Kite enforces RC barriers through a novel fast/slow path mechanism that leverages the absence of failures in the typical case to maximize performance while relying on the slow path for progress. Our evaluation shows that the RDMA-enabled and heavily-multithreaded Kite achieves orders of magnitude better performance than Derecho (a state-of-the-art RDMA-enabled state machine replication system) and significantly outperforms ZAB (the protocol at the heart of Zookeeper). We demonstrate the efficacy of Kite by porting three lock-free shared memory data structures, and showing that Kite outperforms the competition.},
booktitle = {Proceedings of the 25th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {1–16},
numpages = {16},
keywords = {RDMA, availability, consistency, fault tolerance, replication},
location = {San Diego, California},
series = {PPoPP '20}
}


@inproceedings{Kalia-SIGCOMM-2014,
author = {Kalia, Anuj and Kaminsky, Michael and Andersen, David G.},
title = {Using RDMA efficiently for key-value services},
year = {2014},
isbn = {9781450328364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2619239.2626299},
doi = {10.1145/2619239.2626299},
abstract = {This paper describes the design and implementation of HERD, a key-value system designed to make the best use of an RDMA network. Unlike prior RDMA-based key-value systems, HERD focuses its design on reducing network round trips while using efficient RDMA primitives; the result is substantially lower latency, and throughput that saturates modern, commodity RDMA hardware.HERD has two unconventional decisions: First, it does not use RDMA reads, despite the allure of operations that bypass the remote CPU entirely. Second, it uses a mix of RDMA and messaging verbs, despite the conventional wisdom that the messaging primitives are slow. A HERD client writes its request into the server's memory; the server computes the reply. This design uses a single round trip for all requests and supports up to 26 million key-value operations per second with 5μs average latency. Notably, for small key-value items, our full system throughput is similar to native RDMA read throughput and is over 2X higher than recent RDMA-based key-value systems. We believe that HERD further serves as an effective template for the construction of RDMA-based datacenter services.},
booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
pages = {295–306},
numpages = {12},
keywords = {RDMA, ROCE, infiniband, key-value stores},
location = {Chicago, Illinois, USA},
series = {SIGCOMM '14}
}

@ARTICLE{Ma-PDS-2022,
author={Ma, Shaonan and Ma, Teng and Chen, Kang and Wu, Yongwei},
journal={IEEE Transactions on Parallel and Distributed Systems}, 
title={A Survey of Storage Systems in the RDMA Era}, 
year={2022},
volume={33},
number={12},
pages={4395-4409},
keywords={Semantics;Random access memory;Protocols;Hardware;Programming;Performance evaluation;File systems;Network;storage;RDMA;RPC;key-value store;file system;distributed memory;smart NIC},
doi={10.1109/TPDS.2022.3188656}}

