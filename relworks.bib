@inproceedings{Veldema-LCPC-2007,
author = {Veldema, Ronald and Philippsen, Michael},
year = {2007},
month = {10},
pages = {217-231},
title = {Evaluation of RDMA opportunities in an object-oriented DSM},
isbn = {978-3-540-85260-5},
doi = {10.1007/978-3-540-85261-2_15}
}
@article{Cai-VLDB-2018,
author = {Cai, Qingchao and Guo, Wentian and Zhang, Hao and Agrawal, Divyakant and Chen, Gang and Ooi, Beng Chin and Tan, Kian-Lee and Teo, Yong Meng and Wang, Sheng},
title = {Efficient distributed memory management with RDMA and caching},
year = {2018},
issue_date = {July 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3236187.3236209},
doi = {10.14778/3236187.3236209},
abstract = {Recent advancements in high-performance networking interconnect significantly narrow the performance gap between intra-node and inter-node communications, and open up opportunities for distributed memory platforms to enforce cache coherency among distributed nodes. To this end, we propose GAM, an efficient distributed in-memory platform that provides a directory-based cache coherence protocol over remote direct memory access (RDMA). GAM manages the free memory distributed among multiple nodes to provide a unified memory model, and supports a set of user-friendly APIs for memory operations. To remove writes from critical execution paths, GAM allows a write to be reordered with the following reads and writes, and hence enforces partial store order (PSO) memory consistency. A light-weight logging scheme is designed to provide fault tolerance in GAM. We further build a transaction engine and a distributed hash table (DHT) atop GAM to show the ease-of-use and applicability of the provided APIs. Finally, we conduct an extensive micro benchmark to evaluate the read/write/lock performance of GAM under various workloads, and a macro benchmark against the transaction engine and DHT. The results show the superior performance of GAM over existing distributed memory platforms.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {1604–1617},
numpages = {14}
}
@inproceedings{Taranov-ICMD-2021,
author = {Taranov, Konstantin and Di Girolamo, Salvatore and Hoefler, Torsten},
title = {CoRM: Compactable Remote Memory over RDMA},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3452817},
doi = {10.1145/3448016.3452817},
abstract = {Distributed memory systems are becoming increasingly important since they provide a system-scale abstraction where physically separated memories can be addressed as a single logical one. This abstraction enables memory disaggregation, allowing systems as in-memory databases, caching services, and ephemeral storage to be naturally deployed at large scales. While this abstraction effectively increases the memory capacity of these systems, it faces additional overheads for remote memory accesses. To narrow the difference between local and remote accesses, low latency RDMA networks are a key element for efficient memory disaggregation. However, RDMA acceleration poses new obstacles to efficient memory management and particularly to memory compaction: network controllers and CPUs can concurrently access memory, potentially leading to inconsistencies if memory management operations are not synchronized. To ensure consistency, most distributed memory systems do not provide memory compaction and are exposed to memory fragmentation. We introduce CoRM, an RDMA-accelerated shared memory system that supports memory compaction and ensures strict consistency while providing one-sided RDMA accesses. We show that CoRM sustains high read throughput during normal operations, comparable to similar systems not providing memory compaction while experiencing minimal overheads during compaction. CoRM never disrupts RDMA connections and can reduce applications' active memory up to 6x by performing memory compaction.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {1811–1824},
numpages = {14},
keywords = {memory allocation, memory compaction, rdma},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@article{Wang-ACO-2024,
author = {Wang, Zhonghua and Guo, Yixing and Lu, Kai and Wan, Jiguang and Wang, Daohui and Yao, Ting and Wu, Huatao},
title = {Rcmp: Reconstructing RDMA-Based Memory Disaggregation via CXL},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3634916},
doi = {10.1145/3634916},
abstract = {Memory disaggregation is a promising architecture for modern datacenters that separates compute and memory resources into independent pools connected by ultra-fast networks, which can improve memory utilization, reduce cost, and enable elastic scaling of compute and memory resources. However, existing memory disaggregation solutions based on remote direct memory access (RDMA) suffer from high latency and additional overheads including page faults and code refactoring. Emerging cache-coherent interconnects such as CXL offer opportunities to reconstruct high-performance memory disaggregation. However, existing CXL-based approaches have physical distance limitation and cannot be deployed across racks.In this article, we propose Rcmp, a novel low-latency and highly scalable memory disaggregation system based on RDMA and CXL. The significant feature is that Rcmp improves the performance of RDMA-based systems via CXL, and leverages RDMA to overcome CXL’s distance limitation. To address the challenges of the mismatch between RDMA and CXL in terms of granularity, communication, and performance, Rcmp (1) provides a global page-based memory space management and enables fine-grained data access, (2) designs an efficient communication mechanism to avoid communication blocking issues, (3)&nbsp;proposes a hot-page identification and swapping strategy to reduce RDMA communications, and (4) designs an RDMA-optimized RPC framework to accelerate RDMA transfers. We implement a prototype of Rcmp and evaluate its performance by using micro-benchmarks and running a key-value store with YCSB benchmarks. The results show that Rcmp can achieve 5.2\texttimes{} lower latency and 3.8\texttimes{} higher throughput than RDMA-based systems. We also demonstrate that Rcmp can scale well with the increasing number of nodes without compromising performance.},
journal = {ACM Trans. Archit. Code Optim.},
month = jan,
articleno = {15},
numpages = {26},
keywords = {Memory disaggregation, RDMA, CXL}
}
@inproceedings{Dragojevic-NSDI-2014,
author = {Dragojevi\'{c}, Aleksandar and Narayanan, Dushyanth and Hodson, Orion and Castro, Miguel},
title = {FaRM: fast remote memory},
year = {2014},
isbn = {9781931971096},
publisher = {USENIX Association},
address = {USA},
abstract = {We describe the design and implementation of FaRM, a new main memory distributed computing platform that exploits RDMA to improve both latency and throughput by an order of magnitude relative to state of the art main memory systems that use TCP/IP. FaRM exposes the memory of machines in the cluster as a shared address space. Applications can use transactions to allocate, read, write, and free objects in the address space with location transparency. We expect this simple programming model to be sufficient for most application code. FaRM provides two mechanisms to improve performance where required: lock-free reads over RDMA, and support for collocating objects and function shipping to enable the use of efficient single machine transactions. FaRM uses RDMA both to directly access data in the shared address space and for fast messaging and is carefully tuned for the best RDMA performance. We used FaRM to build a key-value store and a graph store similar to Facebook's. They both perform well, for example, a 20-machine cluster can perform 167 million key-value lookups per second with a latency of 31µs.},
booktitle = {Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation},
pages = {401–414},
numpages = {14},
location = {Seattle, WA},
series = {NSDI'14}
}

@INPROCEEDINGS{Endo-IPDRM-2020,
  author={Endo, Wataru and Sato, Shigeyuki and Taura, Kenjiro},
  booktitle={2020 IEEE/ACM Fourth Annual Workshop on Emerging Parallel and Distributed Runtime Systems and Middleware (IPDRM)}, 
  title={MENPS: A Decentralized Distributed Shared Memory Exploiting RDMA}, 
  year={2020},
  volume={},
  number={},
  pages={9-16},
  keywords={Protocols;Merging;Runtime;Coherence;Synchronization;Program processors;Libraries;distributed shared memory;cache coherence protocol;home migration;timestamp based coherence;RDMA},
  doi={10.1109/IPDRM51949.2020.00006}}

@article{Hong-JCST-2019,
  title={Scaling out numa-aware applications with rdma-based distributed shared memory},
  author={Hong, Yang and Zheng, Yang and Yang, Fan and Zang, Bin-Yu and Guan, Hai-Bing and Chen, Hai-Bo},
  journal={Journal of Computer Science and Technology},
  volume={34},
  pages={94--112},
  year={2019},
  publisher={Springer}
}

@article{Jia-ACO-2022,
author = {Jia, Xingguo and Zhang, Jin and Yu, Boshi and Qian, Xingyue and Qi, Zhengwei and Guan, Haibing},
title = {GiantVM: A Novel Distributed Hypervisor for Resource Aggregation with DSM-aware Optimizations},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/3505251},
doi = {10.1145/3505251},
abstract = {We present GiantVM,1 an open-source distributed hypervisor that provides the many-to-one virtualization to aggregate resources from multiple physical machines. We propose techniques to enable distributed CPU and I/O virtualization and distributed shared memory (DSM) to achieve memory aggregation. GiantVM is implemented based on the state-of-the-art type-II hypervisor QEMU-KVM, and it can currently host conventional OSes such as Linux. (1) We identify the performance bottleneck of GiantVM to be DSM, through a top-down performance analysis. Although GiantVM offers great opportunities for CPU-intensive applications to enjoy the aggregated CPU resources, memory-intensive applications could suffer from cross-node page sharing, which requires frequent DSM involvement and leads to performance collapse. We design the guest-level thread scheduler, DaS (DSM-aware Scheduler), to overcome the bottleneck. When benchmarking with NAS Parallel Benchmarks, the DaS could achieve a performance boost of up to 3.5\texttimes{}, compared to the default Linux kernel scheduler. (2) While evaluating DaS, we observe the advantage of GiantVM as a resource reallocation facility. Thanks to the SSI abstraction of GiantVM, migration could be done by guest-level scheduling. DSM allows standby pages in the migration destination, which need not be transferred through the network. The saved network bandwidth is 68\% on average, compared to VM live migration. Resource reallocation with GiantVM increases the overall CPU utilization by 14.3\% in a co-location experiment.},
journal = {ACM Trans. Archit. Code Optim.},
month = mar,
articleno = {20},
numpages = {27},
keywords = {false sharing, distributed shared memory, single system image, virtualization, Resource aggregation}
}

@INPROCEEDINGS{Guedes-WWOSIII-1993,
  author={Guedes, P. and Castro, M.},
  booktitle={Proceedings of IEEE 4th Workshop on Workstation Operating Systems. WWOS-III}, 
  title={Distributed shared object memory}, 
  year={1993},
  volume={},
  number={},
  pages={142-149},
  keywords={Coherence;Programming profession;Yarn;Power system modeling;Prefetching;Hardware;High-speed networks;Workstations;Libraries;Drives},
  doi={10.1109/WWOS.1993.348158}}

@INPROCEEDINGS{Duan-ICDCS-2021,
  author={Duan, Zhuohui and Liu, Haikun and Lu, Haodi and Liao, Xiaofei and Jin, Hai and Zhang, Yu and He, Bingsheng},
  booktitle={2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)}, 
  title={Gengar: An RDMA-based Distributed Hybrid Memory Pool}, 
  year={2021},
  volume={},
  number={},
  pages={92-103},
  keywords={Data centers;Protocols;Costs;Nonvolatile memory;Conferences;Semantics;Random access memory;Distributed Shared Memory;Non-volatile Memory;RDMA;Persistent Memory},
  doi={10.1109/ICDCS51616.2021.00018}}

@inproceedings{Tsai-SOSP-2017,
author = {Tsai, Shin-Yeh and Zhang, Yiying},
title = {LITE Kernel RDMA Support for Datacenter Applications},
year = {2017},
isbn = {9781450350853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132747.3132762},
doi = {10.1145/3132747.3132762},
abstract = {Recently, there is an increasing interest in building data-center applications with RDMA because of its low-latency, high-throughput, and low-CPU-utilization benefits. However, RDMA is not readily suitable for datacenter applications. It lacks a flexible, high-level abstraction; its performance does not scale; and it does not provide resource sharing or flexible protection. Because of these issues, it is difficult to build RDMA-based applications and to exploit RDMA's performance benefits.To solve these issues, we built LITE, a Local Indirection TiEr for RDMA in the Linux kernel that virtualizes native RDMA into a flexible, high-level, easy-to-use abstraction and allows applications to safely share resources. Despite the widely-held belief that kernel bypassing is essential to RDMA's low-latency performance, we show that using a kernel-level indirection can achieve both flexibility and low-latency, scalable performance at the same time. To demonstrate the benefits of LITE, we developed several popular datacenter applications on LITE, including a graph engine, a MapReduce system, a Distributed Shared Memory system, and a distributed atomic logging system. These systems are easy to build and deliver good performance. For example, our implementation of PowerGraph uses only 20 lines of LITE code, while outperforming PowerGraph by 3.5x to 5.6x.},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {306–324},
numpages = {19},
keywords = {RDMA, indirection, low-latency network, network stack},
location = {Shanghai, China},
series = {SOSP '17}
}
@inproceedings{Shamis-SIGMOD-2019,
author = {Shamis, Alex and Renzelmann, Matthew and Novakovic, Stanko and Chatzopoulos, Georgios and Dragojevi\'{c}, Aleksandar and Narayanan, Dushyanth and Castro, Miguel},
title = {Fast General Distributed Transactions with Opacity},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3300069},
doi = {10.1145/3299869.3300069},
abstract = {Transactions can simplify distributed applications by hiding data distribution, concurrency, and failures from the application developer. Ideally the developer would see the abstraction of a single large machine that runs transactions sequentially and never fails. This requires the transactional subsystem to provide opacity (strict serializability for both committed and aborted transactions), as well as transparent fault tolerance with high availability. As even the best abstractions are unlikely to be used if they perform poorly, the system must also provide high performance. Existing distributed transactional designs either weaken this abstraction or are not designed for the best performance within a data center. This paper extends the design of FaRM --- which provides strict serializability only for committed transactions --- to provide opacity while maintaining FaRM's high throughput, low latency, and high availability within a modern data center. It uses timestamp ordering based on real time with clocks synchronized to within tens of microseconds across a cluster, and a failover protocol to ensure correctness across clock master failures. FaRM with opacity can commit 5.4 million neworder transactions per second when running the TPC-C transaction mix on 90 machines with 3-way replication.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {433–448},
numpages = {16},
keywords = {rdma, opacity, multi-version concurrency control, global time, distributed transactions, clock synchronization},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}
@INPROCEEDINGS{Farreras-IPDPS-2009,
author = {Farreras, Montse and Almasi, George and Cascaval, Calin and Cortes, Toni},
title = {Scalable RDMA performance in PGAS languages},
year = {2009},
volume = {},
number = {},
pages = {1-12},
keywords = {Electronics packaging;Large-scale systems;Scalability;Runtime;Parallel programming;Yarn;Delay;Computer architecture;Parallel machines;Productivity},
doi = {10.1109/IPDPS.2009.5161025}
}
