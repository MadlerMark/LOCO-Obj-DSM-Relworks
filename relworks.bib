@inproceedings{Veldema-LCPC-2007,
author = {Veldema, Ronald and Philippsen, Michael},
year = {2007},
month = {10},
pages = {217-231},
title = {Evaluation of RDMA opportunities in an object-oriented DSM},
isbn = {978-3-540-85260-5},
doi = {10.1007/978-3-540-85261-2_15}
}
@article{Cai-VLDB-2018,
author = {Cai, Qingchao and Guo, Wentian and Zhang, Hao and Agrawal, Divyakant and Chen, Gang and Ooi, Beng Chin and Tan, Kian-Lee and Teo, Yong Meng and Wang, Sheng},
title = {Efficient distributed memory management with RDMA and caching},
year = {2018},
issue_date = {July 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3236187.3236209},
doi = {10.14778/3236187.3236209},
abstract = {Recent advancements in high-performance networking interconnect significantly narrow the performance gap between intra-node and inter-node communications, and open up opportunities for distributed memory platforms to enforce cache coherency among distributed nodes. To this end, we propose GAM, an efficient distributed in-memory platform that provides a directory-based cache coherence protocol over remote direct memory access (RDMA). GAM manages the free memory distributed among multiple nodes to provide a unified memory model, and supports a set of user-friendly APIs for memory operations. To remove writes from critical execution paths, GAM allows a write to be reordered with the following reads and writes, and hence enforces partial store order (PSO) memory consistency. A light-weight logging scheme is designed to provide fault tolerance in GAM. We further build a transaction engine and a distributed hash table (DHT) atop GAM to show the ease-of-use and applicability of the provided APIs. Finally, we conduct an extensive micro benchmark to evaluate the read/write/lock performance of GAM under various workloads, and a macro benchmark against the transaction engine and DHT. The results show the superior performance of GAM over existing distributed memory platforms.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {1604–1617},
numpages = {14}
}
@inproceedings{Taranov-ICMD-2021,
author = {Taranov, Konstantin and Di Girolamo, Salvatore and Hoefler, Torsten},
title = {CoRM: Compactable Remote Memory over RDMA},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3452817},
doi = {10.1145/3448016.3452817},
abstract = {Distributed memory systems are becoming increasingly important since they provide a system-scale abstraction where physically separated memories can be addressed as a single logical one. This abstraction enables memory disaggregation, allowing systems as in-memory databases, caching services, and ephemeral storage to be naturally deployed at large scales. While this abstraction effectively increases the memory capacity of these systems, it faces additional overheads for remote memory accesses. To narrow the difference between local and remote accesses, low latency RDMA networks are a key element for efficient memory disaggregation. However, RDMA acceleration poses new obstacles to efficient memory management and particularly to memory compaction: network controllers and CPUs can concurrently access memory, potentially leading to inconsistencies if memory management operations are not synchronized. To ensure consistency, most distributed memory systems do not provide memory compaction and are exposed to memory fragmentation. We introduce CoRM, an RDMA-accelerated shared memory system that supports memory compaction and ensures strict consistency while providing one-sided RDMA accesses. We show that CoRM sustains high read throughput during normal operations, comparable to similar systems not providing memory compaction while experiencing minimal overheads during compaction. CoRM never disrupts RDMA connections and can reduce applications' active memory up to 6x by performing memory compaction.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {1811–1824},
numpages = {14},
keywords = {memory allocation, memory compaction, rdma},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@article{Wang-ACO-2024,
author = {Wang, Zhonghua and Guo, Yixing and Lu, Kai and Wan, Jiguang and Wang, Daohui and Yao, Ting and Wu, Huatao},
title = {Rcmp: Reconstructing RDMA-Based Memory Disaggregation via CXL},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3634916},
doi = {10.1145/3634916},
abstract = {Memory disaggregation is a promising architecture for modern datacenters that separates compute and memory resources into independent pools connected by ultra-fast networks, which can improve memory utilization, reduce cost, and enable elastic scaling of compute and memory resources. However, existing memory disaggregation solutions based on remote direct memory access (RDMA) suffer from high latency and additional overheads including page faults and code refactoring. Emerging cache-coherent interconnects such as CXL offer opportunities to reconstruct high-performance memory disaggregation. However, existing CXL-based approaches have physical distance limitation and cannot be deployed across racks.In this article, we propose Rcmp, a novel low-latency and highly scalable memory disaggregation system based on RDMA and CXL. The significant feature is that Rcmp improves the performance of RDMA-based systems via CXL, and leverages RDMA to overcome CXL’s distance limitation. To address the challenges of the mismatch between RDMA and CXL in terms of granularity, communication, and performance, Rcmp (1) provides a global page-based memory space management and enables fine-grained data access, (2) designs an efficient communication mechanism to avoid communication blocking issues, (3)&nbsp;proposes a hot-page identification and swapping strategy to reduce RDMA communications, and (4) designs an RDMA-optimized RPC framework to accelerate RDMA transfers. We implement a prototype of Rcmp and evaluate its performance by using micro-benchmarks and running a key-value store with YCSB benchmarks. The results show that Rcmp can achieve 5.2\texttimes{} lower latency and 3.8\texttimes{} higher throughput than RDMA-based systems. We also demonstrate that Rcmp can scale well with the increasing number of nodes without compromising performance.},
journal = {ACM Trans. Archit. Code Optim.},
month = jan,
articleno = {15},
numpages = {26},
keywords = {Memory disaggregation, RDMA, CXL}
}
@inproceedings{Dragojevic-NSDI-2014,
author = {Dragojevi\'{c}, Aleksandar and Narayanan, Dushyanth and Hodson, Orion and Castro, Miguel},
title = {FaRM: fast remote memory},
year = {2014},
isbn = {9781931971096},
publisher = {USENIX Association},
address = {USA},
abstract = {We describe the design and implementation of FaRM, a new main memory distributed computing platform that exploits RDMA to improve both latency and throughput by an order of magnitude relative to state of the art main memory systems that use TCP/IP. FaRM exposes the memory of machines in the cluster as a shared address space. Applications can use transactions to allocate, read, write, and free objects in the address space with location transparency. We expect this simple programming model to be sufficient for most application code. FaRM provides two mechanisms to improve performance where required: lock-free reads over RDMA, and support for collocating objects and function shipping to enable the use of efficient single machine transactions. FaRM uses RDMA both to directly access data in the shared address space and for fast messaging and is carefully tuned for the best RDMA performance. We used FaRM to build a key-value store and a graph store similar to Facebook's. They both perform well, for example, a 20-machine cluster can perform 167 million key-value lookups per second with a latency of 31µs.},
booktitle = {Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation},
pages = {401–414},
numpages = {14},
location = {Seattle, WA},
series = {NSDI'14}
}

@INPROCEEDINGS{Endo-IPDRM-2020,
  author={Endo, Wataru and Sato, Shigeyuki and Taura, Kenjiro},
  booktitle={2020 IEEE/ACM Fourth Annual Workshop on Emerging Parallel and Distributed Runtime Systems and Middleware (IPDRM)}, 
  title={MENPS: A Decentralized Distributed Shared Memory Exploiting RDMA}, 
  year={2020},
  volume={},
  number={},
  pages={9-16},
  keywords={Protocols;Merging;Runtime;Coherence;Synchronization;Program processors;Libraries;distributed shared memory;cache coherence protocol;home migration;timestamp based coherence;RDMA},
  doi={10.1109/IPDRM51949.2020.00006}}

@article{Hong-JCST-2019,
author = {Hong, Yang and Zheng, Yang and Yang, Fan and Zang, Bin-Yu and Guan, Hai-Bing and Chen, Hai-Bo},
year = {2019},
month = {01},
pages = {94-112},
title = {Scaling out NUMA-Aware Applications with RDMA-Based Distributed Shared Memory},
volume = {34},
journal = {Journal of Computer Science and Technology},
doi = {10.1007/s11390-019-1901-4}
}

@article{Jia-ACO-2022,
author = {Jia, Xingguo and Zhang, Jin and Yu, Boshi and Qian, Xingyue and Qi, Zhengwei and Guan, Haibing},
title = {GiantVM: A Novel Distributed Hypervisor for Resource Aggregation with DSM-aware Optimizations},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/3505251},
doi = {10.1145/3505251},
abstract = {We present GiantVM,1 an open-source distributed hypervisor that provides the many-to-one virtualization to aggregate resources from multiple physical machines. We propose techniques to enable distributed CPU and I/O virtualization and distributed shared memory (DSM) to achieve memory aggregation. GiantVM is implemented based on the state-of-the-art type-II hypervisor QEMU-KVM, and it can currently host conventional OSes such as Linux. (1) We identify the performance bottleneck of GiantVM to be DSM, through a top-down performance analysis. Although GiantVM offers great opportunities for CPU-intensive applications to enjoy the aggregated CPU resources, memory-intensive applications could suffer from cross-node page sharing, which requires frequent DSM involvement and leads to performance collapse. We design the guest-level thread scheduler, DaS (DSM-aware Scheduler), to overcome the bottleneck. When benchmarking with NAS Parallel Benchmarks, the DaS could achieve a performance boost of up to 3.5\texttimes{}, compared to the default Linux kernel scheduler. (2) While evaluating DaS, we observe the advantage of GiantVM as a resource reallocation facility. Thanks to the SSI abstraction of GiantVM, migration could be done by guest-level scheduling. DSM allows standby pages in the migration destination, which need not be transferred through the network. The saved network bandwidth is 68\% on average, compared to VM live migration. Resource reallocation with GiantVM increases the overall CPU utilization by 14.3\% in a co-location experiment.},
journal = {ACM Trans. Archit. Code Optim.},
month = mar,
articleno = {20},
numpages = {27},
keywords = {false sharing, distributed shared memory, single system image, virtualization, Resource aggregation}
}
